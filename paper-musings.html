<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Peter's Paper Musings</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 font-sans">
    <div class="container mx-auto px-4 py-8">
        <header class="mb-8 max-w-2xl mx-auto">
            <h1 class="text-4xl font-bold text-gray-800 mb-2">Peter's Paper Musings</h1>
            <p class="text-xl text-gray-600 mb-1">Microblogging my favorite systems and database papers</p>
            <p class="text-sm text-gray-500">
                By <a href="https://petereliaskraft.net/" class="text-blue-600 hover:text-blue-800 transition duration-300 ease-in-out">Peter Kraft</a>
            </p>
        </header>
        
        <article class="bg-white rounded-lg shadow-md p-6 mb-8 max-w-2xl mx-auto">
            <h2 class="text-2xl font-semibold text-gray-800 mb-2">
                <a href="http://www.cs.umd.edu/~abadi/papers/abadi-sigmod08.pdf" class="text-blue-600 hover:text-blue-800 transition duration-300 ease-in-out">
                    Column Stores vs. Row Stores: How Different Are They, Really?
                </a>
            </h2>
            <time datetime="2024-09-04" class="text-sm text-gray-500 mb-4 block">2024/09/04</time>
            <div class="prose lg:prose-lg">
                <p class="mb-4">
                    I like this paper from the early days of column stores because it explains how column stores are so effective for analytics and why their benefits are hard to replicate with the traditional row store architecture.
                </p>
                <p class="mb-4">
                    The obvious reason column stores are effective compared to row stores is that they allow queries to only access the columns they need and ignore the others. However, even when that behavior is simulated in row stores (for example, using materialized views containing only the columns needed by a query), column stores perform far better on analytical workloads. Why? The authors find a couple of reasons!
                </p>
                <ol class="list-decimal list-inside mb-4">
                    <li class="mb-2"><strong>Better block iteration.</strong> On analytical queries that evaluate a large number of tuples, a row store needs to iterate through tuples one at a time, extracting data from each tuple to perform predicate evaluation or aggregation. A column store can operate on blocks of values from the same column in a single function call, which takes advantage of CPU caching and parallelism (which is even more true now than it was back then).</li>
                    <li class="mb-2"><strong>Compression.</strong> Column stores put similar data from the same column together, which can enable enormous space savings, and potentially enormous speedups, through compression, for example on a very sparse column. By contrast, a row store has to store whole rows, so even if one column is very sparse or otherwise low-entropy, it is still stored in its entirety.</li>
                    <li class="mb-2"><strong>Late materialization.</strong> Even if a column store needs to return rows to answer a query, it can perform most operations (predicates, joins) only on a handful of columns and then materialize rows later for the data the query returns. This can greatly reduce time spent processing values in columns that are returned but not evaluated.</li>
                </ol>
            </div>
        </article>

        
        <article class="bg-white rounded-lg shadow-md p-6 mb-8 max-w-2xl mx-auto">
            <h2 class="text-2xl font-semibold text-gray-800 mb-2">
                <a href="https://www.usenix.org/system/files/nsdi19-ousterhout.pdf" class="text-blue-600 hover:text-blue-800 transition duration-300 ease-in-out">
                    Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads
                </a>
            </h2>
            <time datetime="2024-09-01" class="text-sm text-gray-500 mb-4 block">2024/09/01</time>
            <div class="prose lg:prose-lg">
                <p class="mb-4">
                    I really like this paper because it identifies a narrow but critical problem in ultra-low-latency systems, proposes a solution, then exhaustively benchmarks it. 
                </p>
                <p class="mb-4">
                    The key observation of this paper is that efficiently serving requests with ultra-low latencies is hard because the operating system allocates cores at a granularity of several milliseconds. Therefore, even with an ultra-fast networking stack that can serve requests in microseconds, when a new request comes in you still need to wait milliseconds to get CPU time.
                </p>
                <p class="mb-4">
                    Prior systems got around that problem through static allocation: they had many cores busy-spin waiting for requests so that if a new request came in, it could be handled immediately. However, that's inefficient, because those cores are doing nothing most of the time.
                </p>
                <p class="mb-4">
                    The main idea in Shenango is to have a single busy-spinning thread, the IOKernel, which both makes core allocation decisions and handles network I/O for a number of application runtimes. The applications all run on user threads on top of kernel threads allocated to Shenango. This setup allows Shenango to efficiently allocate cores between the applications, letting them all serve requests with single-digit microsecond latency even as their relative loads change.
                </p>
                <p class="mb-4">
                    The IOKernel polls the NIC receive queue directly to find packets to forward to applications and polls application egress queues to forward their packets to the NIC. While doing this, it keeps track of how many packets are queued for processing at each application, allocating cores to applications that have queues (and removing cores from applications that don't). Because all applications run in user threads, these core allocation decisions execute in microseconds. There are also a ton of optimizations under the hood, particularly around ensuring application cache locality.
                </p>
                <p class="mb-4">
                    The extensive evaluation section shows incredible performance--serving cache requests for memcached on a single 12-core machine, it could handle 5M requests/second with a median response time of 37 microseconds and a p99.9 of 93 microseconds. Try comparing that to your own stack!
                </p>
                <p class="mb-4">
                    <strong>Main takeaway?</strong> Modern computers can be unbelievably, incredibly fast if we really want them to. We often don't take advantage of this performance because we like having high-level abstractions like OS thread scheduling or an OS network stack, but it's good to know what's possible for when it's really needed.
                </p>
            </div>
        </article>

        <article class="bg-white rounded-lg shadow-md p-6 mb-8 max-w-2xl mx-auto">
            <h2 class="text-2xl font-semibold text-gray-800 mb-2">
                <a href="https://www.pdl.cmu.edu/PDL-FTP/NVM/McAllister-SOSP21.pdf" class="text-blue-600 hover:text-blue-800 transition duration-300 ease-in-out">
                    Kangaroo: Caching Billions of Tiny Objects on Flash
                </a>
            </h2>
            <time datetime="2024-08-25" class="text-sm text-gray-500 mb-4 block">2024/08/25</time>
            <div class="prose lg:prose-lg">
                <p class="mb-4">
                    I really like this paper because it not only presents a clever algorithmic solution to an important systems problem, but also thoroughly evaluates it on real-world data.
                </p>
                <p class="mb-4">
                    The basic challenge here is that many systems (especially, but not only, in social media) want to cache billions of tiny objects (like new posts/messages) on SSDs to improve serving performance. However, existing cache strategies don't work well.
                </p>
                <p class="mb-4">
                    Log-structured caches write objects sequentially and index them in memory, but for tiny objects that index grows too large to fit in memory. Set-associative caches hash objects into "sets" so you don't need an index--you can look up an object's page by its hashed key--but every update requires an entire page write which rapidly degrades the SSD (you can only write to an SSD so many times before it wears out).
                </p>
                <p class="mb-4">
                    This paper's clever idea is to combine the two cache strategies to get their advantages without their disadvantages. They buffer incoming writes in a small log-structured cache, which writes to the SSD efficiently (as you're writing sequentially, so you write a page at a time) but doesn't need much memory (as it's small). Periodically, they export keys to a much larger set-associative cache, doing the exports in large batches to the same set to avoid degrading the SSD. When a read comes in, it first checks the log-structured cache, then goes to the larger set-associative cache.
                </p>
                <p class="mb-4">
                    This design produces a cache that's fast, doesn't require much memory, and doesn't degrade SSDs. The authors prove this with an extensive evaluation on production Facebook traces, verifying all these objectives.
                </p>
                <p class="mb-4">
                    <strong>One big takeaway:</strong> There are only so many ways you can optimize a system, no matter how large or complex. Caching and buffering are basic strategies, but if used cleverly are very effective!
                </p>
            </div>
        </article>
    </div>
</body>
</html>